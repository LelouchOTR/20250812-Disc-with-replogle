# Model configuration for DiscrepancyVAE training
# Configuration for model architecture, training parameters, and optimization

model_config:
  model_params:
    # Latent space configuration
    latent_dim: 64                         # Dimensionality of latent space
    hidden_dims: [256, 128]                # Hidden layer dimensions for GCN encoder

    # Regularization
    dropout_rate: 0.1                      # Dropout rate for all layers
    batch_norm: true                       # Whether to use batch normalization

    # Activation functions
    activation: "relu"             # Activation for encoder: "relu", "leaky_relu", "elu", "gelu"
    output_activation: "linear"            # Output layer activation: "linear", "sigmoid", "softmax"

  loss_params:
    # Primary loss components
    beta: 1.0                              # Weight for KL divergence loss
    reconstruction_loss: 'mse'
    discrepancy_weight: 0.5                # Weight for discrepancy loss between control/perturbed
    graph_reg_weight: 0.001                  # Weight for graph Laplacian regularization
    discrepancy_loss_type: "l2_mean"       # Discrepancy loss type: "l2_mean" or "mmd"
    mmd_gamma: 1.0                         # Gamma parameter for MMD RBF kernel (if used)

# Training configuration
training:
  # Optimization parameters
  batch_size: 16                         # Training batch size
  learning_rate: 0.001                   # Initial learning rate
  weight_decay: 1e-5                     # L2 regularization weight decay

  # Training schedule
  epochs: 1                          # Maximum number of training epochs
  warmup_epochs: 0                      # Number of warmup epochs

  # Learning rate scheduling
  scheduler:
    enabled: true
    type: "cosine"                 # LR scheduler: "cosine", "step", "exponential", "plateau"
    params:
      T_max: 1                          # For cosine scheduler
      eta_min: 1e-6                       # Minimum learning rate

  # Early stopping
  early_stopping_patience: 1            # Epochs to wait before early stopping
  
  # Gradient clipping
  gradient_clip: 1.0                # Maximum gradient norm (None to disable)

  # Validation
  validation_freq: 1                # Validate every N epochs